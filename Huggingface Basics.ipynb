{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Basics\n",
    "Basic usage of huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    pipeline\n",
    "    ,AutoTokenizer\n",
    "    ,TFAutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a pretrained model and tokenizer for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 146kB/s]\n",
      "Downloading: 100%|██████████| 256M/256M [00:09<00:00, 27.9MB/s] \n",
      "2022-05-01 08:48:45.338820: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-01 08:48:45.363206: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 17.8kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.63MB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use classifier on a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the 🤗 Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use classifiers on a list of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment classification on a dataset.\n",
    "\n",
    "TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. The tasks include - irony, hate, offensive, stance, emoji, emotion, and sentiment. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweet_eval/emotion (download: 472.47 KiB, generated: 511.52 KiB, post-processed: Unknown size, total: 984.00 KiB) to /Users/Lauren/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 307kB [00:00, 10.3MB/s]                   ]\n",
      "Downloading data: 6.51kB [00:00, 2.67MB/s]                   .01it/s]\n",
      "Downloading data: 133kB [00:00, 10.6MB/s]                    .73it/s]\n",
      "Downloading data: 2.84kB [00:00, 1.19MB/s]                  3.78it/s]\n",
      "Downloading data: 34.6kB [00:00, 6.47MB/s]                   .05it/s]\n",
      "Downloading data: 748B [00:00, 307kB/s]                     4.22it/s]\n",
      "Downloading data files: 100%|██████████| 6/6 [00:01<00:00,  4.06it/s]\n",
      "Extracting data files: 100%|██████████| 6/6 [00:00<00:00, 1047.70it/s]\n",
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweet_eval downloaded and prepared to /Users/Lauren/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"tweet_eval\", name='emotion', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9921145439147949},\n",
       " {'label': 'NEGATIVE', 'score': 0.9914141297340393},\n",
       " {'label': 'POSITIVE', 'score': 0.9987362027168274},\n",
       " {'label': 'NEGATIVE', 'score': 0.6745527982711792}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = dataset[\"text\"]\n",
    "classifier(files[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Example\n",
    "Using BillSum dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    pipeline\n",
    "    ,AutoTokenizer\n",
    "    ,DataCollatorForSeq2Seq\n",
    "    ,TFAutoModelForSeq2SeqLM\n",
    "    ,create_optimizer\n",
    "    ,AdamWeightDecay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 3.62kB [00:00, 370kB/s]                    \n",
      "Downloading metadata: 1.75kB [00:00, 668kB/s]                  \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset billsum/default (download: 64.14 MiB, generated: 259.80 MiB, post-processed: Unknown size, total: 323.94 MiB) to /Users/Lauren/.cache/huggingface/datasets/billsum/default/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 67.3M/67.3M [00:55<00:00, 1.20MB/s]\n",
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset billsum downloaded and prepared to /Users/Lauren/.cache/huggingface/datasets/billsum/default/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 1938 of the Civil Code is amended to read:\\n1938.\\n(a) A commercial property owner or lessor shall state on every lease form or rental agreement executed on or after January 1, 2016, whether or not the subject premises have undergone inspection by a Certified Access Specialist (CASp).\\n(b) If the subject premises have undergone inspection by a CASp and, to the best of the commercial property owner’s or lessor’s knowledge, there have been no modifications or alterations completed or commenced between the date of the inspection and the date of the lease or rental agreement which have impacted the subject premises’ compliance with construction-related accessibility standards, the commercial property owner or lessor shall provide, prior to execution of the lease or rental agreement, a copy of any report prepared by the CASp with an agreement from the prospective lessee or tenant that information in the report shall remain confidential, except as necessary for the tenant to complete repairs and corrections of violations of construction-related accessibility standards that the lessee or tenant agrees to make.\\n(c) If the subject premises have been issued an inspection report by a CASp, as described in paragraph (1) of subdivision (a) of Section 55.53, indicating that it meets applicable standards, as defined in paragraph (4) of subdivision (a) of 55.52, the commercial property owner or lessor shall provide a copy of the current disability access inspection certificate and any inspection report to the lessee or tenant not already provided pursuant to subdivision (b) within seven days of the date of the execution of the lease form or rental agreement.\\n(d) If the subject premises have not been issued a disability access inspection certificate, as described in subdivision (e) of Section 55.53, the commercial property owner or lessor shall state the following on the lease form or rental agreement:\\n\\n\\n“A Certified Access Specialist (CASp) can inspect the subject premises and determine whether the subject premises comply with all of the applicable construction-related accessibility standards under state law. Although state law does not require a CASp inspection of the subject premises, the commercial property owner or lessor may not prohibit the lessee or tenant from obtaining a CASp inspection of the subject premises for the occupancy or potential occupancy of the lessee or tenant, at the lessee’s or tenant’s expense, if requested by the lessee or tenant. The parties shall mutually agree on the arrangements for the time and manner of the CASp inspection.”\\nSEC. 2.\\nSection 4459.8 of the Government Code is amended to read:\\n4459.8.\\n(a) The certification authorized by Section 4459.5 is effective for three years from the date of initial certification and expires if not renewed. The State Architect, upon consideration of any factual complaints regarding the work of a certified access specialist or of other relevant information, may suspend certification or deny renewal of certification.\\n(b) (1) The State Architect shall require each applicant for certification as a certified access specialist to do both of the following:\\n(A) Pay fees, including an application and course fee and an examination fee, at a level sufficient to meet the costs of application processing, registration, publishing a list, and other activities that are reasonably necessary to implement and administer the certified access specialist program.\\n(B) Provide to the State Architect the name of the city, county, or city and county in which the applicant intends to provide services.\\n(2) The State Architect shall require each applicant for renewal of certification to do both of the following:\\n(A) Pay a fee sufficient to cover the reasonable costs of reassessing qualifications of renewal applicants.\\n(B) Provide to the State Architect the name of the city, county, or city and county in which the applicant has provided services since the last day of certification by the State Architect.\\n(3) The State Architect shall periodically review his or her schedule of fees to ensure that the fees for certification are not excessive while covering the costs to administer the certified access specialist program. The application fee for a California licensed architect, landscape architect, civil engineer, or structural engineer shall not exceed two hundred fifty dollars ($250).\\n(c) All fees collected pursuant to this section shall be deposited into the Certified Access Specialist Fund, which is hereby created in the State Treasury. Notwithstanding Section 13340, this fund is continuously appropriated without regard to fiscal years for use by the State Architect to implement Sections 4459.5 to 4459.8, inclusive.\\n(d) The State Architect shall post on his or her Internet Web site the name of the city, county, or city and county in which each certified access specialist provides or intends to provide services.\\nSEC. 3.\\nSection 8299.06 of the Government Code is amended to read:\\n8299.06.\\n(a) A priority of the commission shall be the development and dissemination of educational materials and information to promote and facilitate disability access compliance.\\n(b) The commission shall work with other state agencies, including the Division of the State Architect and the Department of Rehabilitation, to develop educational materials and information for use by businesses to understand their obligations to provide disability access and to facilitate compliance with construction-related accessibility standards.\\n(c) The commission shall develop and make available on its Internet Web site, or make available on its Internet Web site if developed by another governmental agency, including Americans with Disabilities Act centers, toolkits or educational modules to assist a California business to understand its obligations under the law and to facilitate compliance with respect to the top 10 alleged construction-related violations, by type, as specified in subdivision (a) of Section 8299.08. Upon completion of this requirement, the commission shall develop and make available on its Internet Web site, or work with another agency to develop, other toolkits or educational modules that would educate businesses of the accessibility requirements and to facilitate compliance with that requirement.\\n(d) The commission shall post the following on its Internet Web site:\\n(1) Educational materials and information that will assist building owners, tenants, building officials, and building inspectors to understand the disability accessibility requirements and to facilitate compliance with disability access laws. The commission shall at least annually review the educational materials and information on disability access requirements and compliance available on the Internet Web sites of other local, state, or federal agencies, including Americans with Disabilities Act centers, to augment the educational materials and information developed by the commission.\\n(2) A link to the Internet Web site of the Division of the State Architect’s Certified Access Specialist (CASp) Program to assist building owners and tenants in locating or hiring a CASp.\\n(e) The commission shall, to the extent feasible, coordinate with other state agencies and local building departments to ensure that information provided to the public on disability access requirements is uniform and complete, and make its educational materials and information available to those agencies and departments.\\n(f) The commission shall establish a permanent legislative outreach coordinator position and a permanent educational outreach coordinator position.\\nSEC. 4.\\nNo reimbursement is required by this act pursuant to Section 6 of Article XIII\\u2009B of the California Constitution because a local agency or school district has the authority to levy service charges, fees, or assessments sufficient to pay for the program or level of service mandated by this act, within the meaning of Section 17556 of the Government Code.',\n",
       " 'summary': '(1)\\xa0Existing law requires the State Architect to establish and publicize a program for the voluntary certification by the state of any person who meets specified criteria as a Certified Access Specialist (CASp). Existing law requires each applicant for CASp certification or renewal to pay certain fees, and requires the State Architect to periodically review those fees, as specified. Existing law provides for the deposit of those fees into the Certified Access Specialist Fund, which is continuously appropriated for use by the State Architect to implement the CASp program.\\nThis bill would require applicants for CASp certification or renewal to additionally provide to the State Architect the name of the city, county, or city and county in which the applicant intends to provide or has provided services, and would require the State Architect to post that information on his or her Internet Web site.\\n(2)\\xa0Existing law requires a commercial property owner or lessor to state on every lease form or rental agreement executed on or after July 1, 2013, whether the property has been determined by a CASp to meet all applicable construction-related accessibility standards.\\nThis bill, for every lease form or rental agreement executed on or after January 1, 2016, would require the commercial property owner or lessor to provide the lessee or tenant with a current disability access inspection certificate and inspection report or a copy of a CASp inspection report, as specified, or would require a statement on the form or agreement that, upon request of the lessee or tenant, the property owner may not prohibit a CASp inspection of the subject premises at the lessee’s or tenant’s expense and that the parties must mutually agree on the arrangements for the time and manner of the inspection.\\n(3)\\xa0Existing law establishes the California Commission on Disability Access for purposes of developing recommendations to enable persons with disabilities to exercise their right to full and equal access to public facilities and facilitating business compliance with applicable state and federal laws and regulations. Existing law sets forth the powers and duties of the commission, including, but not limited to, developing educational materials and information for businesses, building owners, tenants, and building officials, posting that information on the commission’s Internet Web site, and coordinating with other state agencies and local building departments to ensure that information provided to the public on disability access requirements is uniform and complete. Existing law provides that those provisions shall not remain operative unless funds are appropriated for those purposes.\\nThis bill would additionally require the commission to provide a link on its Internet Web site to the Internet Web site of the Division of the State Architect’s CASp certification program and to make the commission’s educational materials and information available to other state agencies and local building departments.\\n(4)\\xa0The California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\\nThis bill would provide that no reimbursement is required by this act for a specified reason.',\n",
       " 'title': 'An act to amend Section 1938 of the Civil Code, and to amend Sections 4459.8 and 8299.06 of the Government Code, relating to disability access.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'summary', 'title'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 451kB/s]\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 922kB/s] \n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 2.05MB/s]\n"
     ]
    }
   ],
   "source": [
    "#load T5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    The preprocessing function needs to:\n",
    "    1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "    2. Use a context manager with the as_target_tokenizer() function to parallelize tokenization of inputs and labels.\n",
    "    3. Truncate sequences to be no longer than the maximum length set by the max_length parameter.\n",
    "    From: https://huggingface.co/docs/transformers/tasks/summarization#preprocess\n",
    "    \"\"\"\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 🤗 Datasets map function to apply the preprocessing function over the entire dataset. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.42s/ba]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/ba]\n"
     ]
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model class (with a sequence-to-sequence language modeling head) from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 231M/231M [00:11<00:00, 20.9MB/s] \n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DataCollatorForSeq2Seq to create a batch of examples. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the tokenizer function by setting `padding=True`, dynamic padding is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}), model=<transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration object at 0x7ff7f0ab9f40>, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='tf')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to the tf.data.Dataset format with to_tf_dataset. Specify inputs and labels in columns, whether to shuffle the dataset order, batch size, and the data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = tokenized_billsum[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = tokenized_billsum[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an optimizer function, learning rate schedule, and some training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `fit` to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "61/61 [==============================] - 3723s 60s/step - loss: 3.6437 - val_loss: 2.8856\n",
      "Epoch 2/3\n",
      "61/61 [==============================] - 3112s 51s/step - loss: 3.0093 - val_loss: 2.6413\n",
      "Epoch 3/3\n",
      "61/61 [==============================] - 3094s 51s/step - loss: 2.8309 - val_loss: 2.5230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7f8ac7640>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory='./my-t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ./my-t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pretmodel = TFAutoModelForSeq2SeqLM.from_pretrained(\"./my-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=pretmodel, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 45. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'I love the song I Just Wanna Be a Pickle by Natalie Burdick . Get my booty in that brine.'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"I love the song I Just Wanna Be a Pickle by Natalie Burdick. She sings, I just wanna be a pickle. Get my booty in that brine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb3abc02fa632af4b5624485f38b93ba6adc7e202dc5d97b126ed46dea4c2a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
