{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BLOOM\n",
    "September 2022\n",
    "\n",
    "Test using BLOOM model through huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BloomModel,\n",
    "    BloomConfig, \n",
    "    BloomTokenizerFast,\n",
    "    BloomForCausalLM,\n",
    "    BloomForSequenceClassification,\n",
    "    BloomForTokenClassification\n",
    ")\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Bloom configuration\n",
    "configuration = BloomConfig()\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = BloomModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04047894477844238,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 222,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea406d2d197a473a9713af83ca6b9974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024224042892456055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 14500438,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbf304519a64598bd6990803a753997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/13.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021235942840576172,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 85,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788becef9a474717a8fe0b9216c18d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017394065856933594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 710,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b68077832204ecfa5193887ee10cde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02844381332397461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1118531895,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fbb02a6dc24491b1cb1b2ae0b4703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomModel.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9988e+00,  2.1394e+00, -3.3615e+00,  ..., -1.0710e+03,\n",
       "          -2.3329e+00,  6.6761e+00],\n",
       "         [-2.7488e+00, -1.3973e+00, -3.2516e+00,  ..., -9.0800e+02,\n",
       "          -1.8817e-01,  2.5218e+00],\n",
       "         [ 5.5479e-01, -2.2881e+00, -4.1895e-01,  ..., -9.4320e+02,\n",
       "           2.3407e+00,  5.4891e+00],\n",
       "         [ 1.3067e+00, -3.3615e+00, -1.9271e+00,  ..., -1.0148e+03,\n",
       "           3.0082e+00,  6.0203e+00],\n",
       "         [-2.3621e+00, -6.1651e+00, -1.9696e+00,  ..., -9.6746e+02,\n",
       "           2.2044e+00,  4.4250e+00],\n",
       "         [ 1.9945e+00, -6.8697e+00,  8.5640e-01,  ..., -9.9383e+02,\n",
       "           1.5745e+00,  8.3419e+00]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLoomForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 4.195248126983643 to Sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Lauren/Documents/Python/nlp_practice/Test BLOOM.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Lauren/Documents/Python/nlp_practice/Test%20BLOOM.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Lauren/Documents/Python/nlp_practice/Test%20BLOOM.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Lauren/Documents/Python/nlp_practice/Test%20BLOOM.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39;49mdecode(outputs[\u001b[39m0\u001b[39;49m]))\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/nlp_practice/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3367\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3364\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3367\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3368\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3369\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3370\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3371\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/nlp_practice/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    547\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 548\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[1;32m    551\u001b[0m     clean_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 4.195248126983643 to Sequence"
     ]
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[346.8376, 349.7458, 361.8978,  ..., 204.2956, 204.2954, 204.2904],\n",
       "         [333.0381, 333.2141, 343.7316,  ..., 197.0953, 197.0948, 197.0892],\n",
       "         [401.7427, 404.6340, 417.7802,  ..., 208.0413, 208.0413, 208.0355],\n",
       "         [402.8060, 404.7499, 423.0336,  ..., 207.9611, 207.9613, 207.9560],\n",
       "         [408.2845, 405.2424, 424.2576,  ..., 206.7770, 206.7772, 206.7714],\n",
       "         [410.1431, 410.2028, 430.8823,  ..., 208.3094, 208.3098, 208.3039]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Illinois is located in the city of Chicago. The capital of Illinois is located in the city of Chicago. The\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "prompt = \"The capital of Illinois is\"\n",
    "input_token_ids = tokenizer(prompt, return_tensors='pt')\n",
    "output = model.generate(**input_token_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love my dog because he is my best friend. He is my best friend because he is my best friend. He is\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "prompt = \"I love my dog because\"\n",
    "input_token_ids = tokenizer(prompt, return_tensors='pt')\n",
    "output = model.generate(**input_token_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    39, 216251,   1306,    368,   7733,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BloomForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomForSequenceClassification.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[93.1177, 92.4969]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BloomForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForTokenClassification were not initialized from the model checkpoint at bigscience/bloom-350m and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\")\n",
    "model = BloomForTokenClassification.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "predicted_tokens_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_tokens_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb3abc02fa632af4b5624485f38b93ba6adc7e202dc5d97b126ed46dea4c2a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
